---
layout: post
title:  "Paper Read 7 - Attention Is All You Need"
date:   2024-2-1 1:18:00 +0800
tags:
- Embodied Intelligence
- NLP
toc:  true
math: true

---

The emergence of LLM has advanced the field of robotics. These models can learn rich linguistic knowledge and semantic representations by being pre-trained on **large-scale textual data**. These models can then be fine-tuned to adapt to specific tasks or domains. Therefore, in this post, I am going to document my learning process of **embodied intelligence**. Since I only know about the robotics field before, and the main body of the paper is in the field of NLP, I decided to record in my native Chinese language in order to make it easier for me to understand.

This series is expected to consist of a relatively small number of posts, with the short-term end goal of sustaining my reading of the Google PaLM-E article. 

## **Attention**

简单的举例理解：[注意力机制的本质|Self-Attention|Transformer|QKV矩阵_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1dt4y1J7ov/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&vd_source=8a1fa40e08f3ead438b9bc465bd04915)

详细课程地址：[10.【李宏毅机器学习2021】自注意力机制 (Self-attention) (上)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1v3411r78R?p=1&vd_source=8a1fa40e08f3ead438b9bc465bd04915)， 课程笔记[self-attention笔记](https://go2schooi.github.io/self_v7.pdf). 







## **Transformer**

[13.【李宏毅机器学习2021】Transformer (下)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1v3411r78R/?p=4&spm_id_from=pageDriver&vd_source=8a1fa40e08f3ead438b9bc465bd04915)